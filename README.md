# Knowledge Distillation on CIFAR-10 with Precomputed Multi-View Teacher Logits

This repository implements knowledge distillation (KD) from a ViT teacher to a smaller student model on CIFAR-10.  
Instead of computing teacher logits on-the-fly during training (which is slow), we **precompute teacher logits for multiple augmented views** of each image â€” then use them during student training to simulate multi-view distillation.

> âœ… Why? To reduce overfitting from single-view KD while avoiding expensive real-time teacher inference.

---

## ğŸ“ Project Structure
â”œâ”€â”€ cache/ # Precomputed teacher logits & augmented data
â”‚ â”œâ”€â”€ cifar10_train_augmented_k*.pth # Augmented images (k=0~4)
â”‚ â”œâ”€â”€ teacher_logits_k*.pth # Teacher logits for each view
â”‚ â””â”€â”€ teacher_vit_small_cifar10.pth # Finetuned ViT teacher model
â”œâ”€â”€ data/ # CIFAR-10 raw data (downloaded separately)
â”œâ”€â”€ results/ # Training metrics & final checkpoints
â”‚ â”œâ”€â”€ distill_multiview.json # Final accuracy/metrics
â”‚ â”œâ”€â”€ finetuned_teacher.json # Teacher fine-tuning metrics
â”‚ â”œâ”€â”€ student_distill_multiview.pth # Best distilled student model
â”‚ â”œâ”€â”€ student_final.pth # Final student checkpoint
â”‚ â””â”€â”€ student_scratch.json # Student trained from scratch (baseline)
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ distill_train.py # Main script: trains student with precomputed logits
â”œâ”€â”€ finetune_teacher.py # Fine-tunes ViT teacher on CIFAR-10
â”œâ”€â”€ precompute_augmented_data.py # Generates k=5 augmented views per image
â”œâ”€â”€ precompute_teacher_logits.py # Computes teacher logits for all augmented views
â”œâ”€â”€ scratch_train.py # Trains student from scratch (no KD)
â”œâ”€â”€ student_vit.py # Defines student model (ViT)
â””â”€â”€ utils.py # Helper functions

## âš™ï¸ Setup & Usage
Precompute Teacher Logits (One-Time Setup)

First, train/fine-tune the teacher model:
python finetune_teacher.py

Then generate 5 augmented views per image:
python precompute_augmented_data.py

Finally, compute teacher logits for all views:
python precompute_teacher_logits.py
â†’ This will populate cache/ with .pth files.

Train with distillation using precomputed multi-view logits:
python distill_train.py

Or train from scratch (baseline):
python scratch_train.py

Results
Check results/distill_multiview.json and results/student_scratch.json for final accuracy.

## âœ¨Final Results
| Method             | StudentAcc(%) | TeacherAcc(%) |
|--------------------|---------------|---------------|
| Baseline (CE)      | 88.6          | â€“             |
| KD (Single View)   | 71.1          | 92.8          |
| KD (Multi-View)    | **87.1**      | 92.8          |

> ğŸ’¡ Multi-view distillation brings student performance close to baseline â€” while still benefiting from teacher knowledge!

## ğŸ§  Key Design Choices
Precomputation: Teacher logits are computed once and reused â†’ faster training.
Multi-view: We generate 5 different augmentations per image â†’ student sees â€œdifferent viewsâ€ of same sample â†’ reduces overfitting.
No online teacher: Avoids GPU memory pressure from running teacher during student training.
